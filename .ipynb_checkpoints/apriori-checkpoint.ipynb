{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data took 0.021308183670043945 seconds.\n",
      "subset took 0.5459275245666504 seconds.\n",
      "subset took 33.100677490234375 seconds.\n",
      "subset took 1.7360427379608154 seconds.\n",
      "subset took 0.367229700088501 seconds.\n",
      " Beef Burger(51) ---> Burgers(541) - conf(0.7647058823529411)\n",
      " Veg Burger(69) --->  Burgers(775) - conf(0.5072463768115942)\n",
      " Chicken Boneless Biryani(70) --->  Vegetable Biryani(268) - conf(0.5428571428571428)\n",
      " Chicken Burger(89) --->  Pasta(1265) - conf(0.5393258426966292)\n",
      "Shawarma(82) --->  Chicken Grill(204) - conf(0.524390243902439)\n",
      " Thukpa(51) --->  Chicken Momo(80) - conf(0.6078431372549019)\n",
      " Chicken Roll(80) ---> Rolls(204) - conf(0.575)\n",
      " Craft Beer(88) --->  Cocktails(823) - conf(0.6818181818181818)\n",
      " Jumbo Prawns(52) --->  Cocktails(823) - conf(0.7115384615384616)\n",
      " Sangria(120) --->  Cocktails(823) - conf(0.6083333333333333)\n",
      "Beer(151) --->  Cocktails(823) - conf(0.5761589403973509)\n",
      " Kesari Bath(47) --->  Filter Coffee(190) - conf(0.6595744680851063)\n",
      " Wings(56) --->  Fries(455) - conf(0.5714285714285714)\n",
      " Hot Chocolate Fudge(67) --->  Waffles(241) - conf(0.5373134328358209)\n",
      " Hot Dog(47) ---> Burgers(541) - conf(0.723404255319149)\n",
      "Vada(62) --->  Idli(164) - conf(0.5161290322580645)\n",
      "Vada(62) --->  Masala Dosa(259) - conf(0.6774193548387096)\n",
      "Hyderabadi Biryani(61) --->  Mutton Biryani(407) - conf(0.5573770491803278)\n",
      " Prawn Ghee Roast(48) --->  Neer Dosa(162) - conf(0.6666666666666666)\n",
      " Neer Dosa(162) ---> Sea Food(182) - conf(0.5061728395061729)\n",
      " Beer,  Pizza(78) --->  Nachos(573) - conf(0.5256410256410257)\n",
      " Beer,  Nachos(73) --->  Pizza(834) - conf(0.5616438356164384)\n",
      " Burgers,  Cocktails(74) --->  Pasta(1265) - conf(0.581081081081081)\n",
      " Burgers, Cocktails(58) --->  Nachos(573) - conf(0.5344827586206896)\n",
      " Burgers,  Sandwich(80) --->  Pasta(1265) - conf(0.5875)\n",
      " Burgers, Cocktails(58) --->  Pizza(834) - conf(0.5862068965517241)\n",
      " Cappuccino,  Sandwiches(58) ---> Coffee(441) - conf(0.5344827586206896)\n",
      " Chicken Biryani,  Mocktails(46) --->  Cocktails(823) - conf(0.7391304347826086)\n",
      " Chicken Biryani,  Cocktails(53) --->  Mocktails(692) - conf(0.6415094339622641)\n",
      " Cocktails,  Nachos(108) --->  Mocktails(692) - conf(0.5462962962962963)\n",
      " Pizza, Beer(64) --->  Cocktails(823) - conf(0.734375)\n",
      " Cocktails, Beer(87) --->  Pizza(834) - conf(0.5402298850574713)\n",
      " Coffee, Pizza(39) --->  Pasta(1265) - conf(0.8461538461538461)\n",
      " Fries, Cocktails(34) --->  Pizza(834) - conf(0.9117647058823529)\n",
      " Mocktails, Pizza(49) --->  Pasta(1265) - conf(0.7755102040816326)\n",
      " Nachos, Pizza(86) --->  Pasta(1265) - conf(0.6046511627906976)\n",
      " Nachos, Cocktails(67) --->  Pizza(834) - conf(0.5373134328358209)\n",
      " Pizza, Coffee(57) --->  Pasta(1265) - conf(0.6842105263157895)\n",
      " Sandwiches, Pizza(62) --->  Pasta(1265) - conf(0.5645161290322581)\n",
      " Sandwiches, Pasta(60) --->  Sandwich(401) - conf(0.6)\n",
      " Sandwich, Pasta(41) --->  Sandwiches(639) - conf(0.8780487804878049)\n",
      "No of rules: 41 No of itemsets: 740\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "import import_ipynb\n",
    "from hash_tree import Tree, generate_subsets\n",
    "from timing_wrapper import timeit\n",
    "\n",
    "# Important variables\n",
    "MINSUP = 30 # Minimum support\n",
    "HASH_DENOMINATOR = 10 # Denominator of the hash function\n",
    "MIN_CONF = 0.5# Minimum confidence\n",
    "\n",
    "@timeit\n",
    "def load_data(path):\n",
    "    '''\n",
    "    Function to read itemsets from file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "    path to file containinng transactions.\n",
    "    Returns\n",
    "    ----------\n",
    "    transactions : list\n",
    "    list containing all transactions. Each transaction is a list of\n",
    "    items present in that transaction.\n",
    "    items : list\n",
    "    list containing all the unique items.\n",
    "    '''\n",
    "    items = []\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        transactions = list(reader)\n",
    "    for x in transactions:\n",
    "        items.extend(x)\n",
    "    items=sorted(set(items))\n",
    "    return transactions, items\n",
    "\n",
    "def create_map(items):\n",
    "    '''\n",
    "    Function to map unique items to integers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    items : list\n",
    "    list of unique items.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    map_ : dict\n",
    "    Items --> integers mapping.\n",
    "    reverse_map : dict\t\n",
    "    Integers --> items mapping.\n",
    "    '''\n",
    "    map_ = {x:i for i,x in enumerate(items)}\n",
    "    reverse_map = {i:x for i,x in enumerate(items)}\n",
    "    return map_, reverse_map\n",
    "\n",
    "def applymap(transaction, map_):\n",
    "    '''\n",
    "    Function to apply mapping to items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    transaction : list\n",
    "    single transaction.\n",
    "    map_ : dict\n",
    "        mapping.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    ret : dict\n",
    "        mapped transaction.\n",
    "    '''\n",
    "    ret = []\n",
    "    for item in transaction:\n",
    "        ret.append(map_[item])\n",
    "    return ret\n",
    "\n",
    "#@timeit\n",
    "def apriori_gen(l_prev):\n",
    "    '''\n",
    "    Function to generate c(k+1) from l(k).\n",
    "    \n",
    "    This function has been implemented as presented in Introduction to Data \n",
    "    Mining,Tan Pang-Ning et al, section 6.2.3\n",
    "    Parameters\n",
    "    ----------\n",
    "    l_prev : list\n",
    "        l(k)\n",
    "    Returns\n",
    "    ----------\n",
    "    c_curr : list\n",
    "        c(k+1).\n",
    "    '''\n",
    "    n = len(l_prev)\n",
    "    c_curr = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            temp_a = l_prev[i]\n",
    "            temp_b = l_prev[j]\n",
    "            if temp_a[:-1] == temp_b[:-1]:\n",
    "                temp_c = []\n",
    "                temp_c.extend(temp_a)\n",
    "                temp_c.append(temp_b[-1])\n",
    "                temp_c=sorted(temp_c)\n",
    "                c_curr.append(temp_c)\n",
    "    return c_curr\n",
    "\n",
    "# Brute force subset generation and support counting\n",
    "# @timeit\n",
    "# def subset(c_list, transactions):\n",
    "# \tcandidate_counts={}\n",
    "# \tfor transaction in transactions:\n",
    "# \t\tfor candidate in c_list:\n",
    "# \t\t\tif set(candidate).issubset(set(transaction)):\n",
    "# \t\t\t\tcandidate_counts[tuple(candidate)] = candidate_counts.get(tuple(candidate), 0)\n",
    "# \t\t\t\tcandidate_counts[tuple(candidate)] += 1\n",
    "# \treturn candidate_counts\n",
    "\n",
    "@timeit\n",
    "def subset(c_list, transactions):\n",
    "\t'''\n",
    "\tFunction to get support counts of candidates.\n",
    "\tParameters\n",
    "\t----------\n",
    "\ttransaction : list\n",
    "\t    single transaction.\n",
    "\tmap_ : dict\n",
    "\t    mapping.\n",
    "\t'''\n",
    "\tcandidate_counts={}\n",
    "\tt=Tree(c_list, k=HASH_DENOMINATOR, max_leaf_size=100)\n",
    "\tfor transaction in transactions:\n",
    "\t\tsubsets =generate_subsets(transaction, len(c_list[0]))\n",
    "\t\tfor sub in subsets:\n",
    "\t\t\tt.check(sub, update=True)\n",
    "\tfor candidate in c_list:\n",
    "\t\tcandidate_counts[tuple(candidate)] = t.check(candidate, update=False)\n",
    "\treturn candidate_counts\n",
    "\n",
    "def frequent_itemset_generation(data_path):\n",
    "\t'''\n",
    "\tFunction to read data and generate frequent itemsets using the Apriori algorithm.\n",
    "\tParameters\n",
    "\t----------\n",
    "\tdata_path : string\n",
    "\t    path to file containing transactions.\n",
    "\tReturns\n",
    "\t----------\n",
    "\tL_final : list\n",
    "\t\tlist of dictionaries containing the final L set.\n",
    "\t'''\n",
    "\n",
    "\t# Uncomment the following lines to load saved pickle file and avoid the extra time required\n",
    "\t# for frequent itemset generation.\n",
    "\t# if 'l_final.pkl' in os.listdir('.'):\n",
    "\t# \treturn pickle.load(open('l_final.pkl', 'rb'))\n",
    "\ttransactions, items = load_data(data_path)\n",
    "\tmap_, reverse_map = create_map(items)\n",
    "\tpickle.dump(reverse_map, open('reverse_map.pkl', 'wb+'))\n",
    "\tone_itemset = [[itemset] for itemset in items]\n",
    "\titems_mapped = [applymap(itemset, map_) for itemset in one_itemset]\n",
    "\ttransactions_mapped = [applymap(transaction, map_) for transaction in transactions]\n",
    "\t\n",
    "\ttemp_l_current = subset(items_mapped, transactions_mapped)\n",
    "\tl_current={}\n",
    "\tfor t in temp_l_current.keys():\n",
    "\t\tif temp_l_current[t] > MINSUP:\n",
    "\t\t\tl_current[tuple(t)] = temp_l_current[t]\n",
    "\tL_final = []\n",
    "\tL_final.append(l_current)\n",
    "\n",
    "\twhile(len(l_current)):\n",
    "\t\tc_current = apriori_gen(list(l_current.keys()))\n",
    "\t\tif len(c_current):\n",
    "\t\t\tC_t = subset(c_current, transactions_mapped)\n",
    "\t\t\tl_current = {}\n",
    "\t\t\tfor c in C_t.keys():\n",
    "\t\t\t\tif C_t[c] > MINSUP:\n",
    "\t\t\t\t\tl_current[tuple(sorted(c))] = C_t[c]\n",
    "\t\t\tif len(l_current):\n",
    "\t\t\t\tL_final.append(l_current)\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\tpickle.dump(L_final, open('l_final.pkl', 'wb+'))\n",
    "\treturn L_final\n",
    "\n",
    "def generate_rules(frequent_items):\n",
    "\t'''\n",
    "\tFunction to generate rules from frequent itemsets.\n",
    "\tParameters\n",
    "\t----------\n",
    "\tfrequent_items : list\n",
    "\t    list containing all frequent itemsets.\n",
    "\tReturns\n",
    "\t----------\n",
    "\trules : list\n",
    "\t\tlist of generated rules.\n",
    "\trules is stored in the following format-\n",
    "\t[(X, Y), (X,Y)]\n",
    "\t'''\n",
    "\trules=[]\n",
    "\tfor k_itemset in frequent_items:\n",
    "\t\tk=len(list(k_itemset.keys())[0])\n",
    "\t\tif k==1: # No rules can be generated using 1 itemsets\n",
    "\t\t\tcontinue\n",
    "\t\tfor itemset, support in k_itemset.items():\n",
    "\t\t\tH_curr=[[x] for x in itemset]\n",
    "\t\t\tto_remove=[]\n",
    "\t\t\tfor h in H_curr:\n",
    "\t\t\t\tX=tuple(sorted(set(itemset)-set(h)))\n",
    "\t\t\t\tY=tuple(sorted(h))\n",
    "\t\t\t\tconfidence = support / (frequent_items[k-2][X])\n",
    "\t\t\t\tif confidence > MIN_CONF:\n",
    "\t\t\t\t\trule=[]\n",
    "\t\t\t\t\trule.append(X)\n",
    "\t\t\t\t\trule.append(Y)\n",
    "\t\t\t\t\trules.append({tuple(rule):confidence})\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tto_remove.append(h)\n",
    "\n",
    "\t\t\tH_curr=[x for x in H_curr if x not in to_remove]\n",
    "\n",
    "\t\t\tfor m in range(1,k-1):\n",
    "\t\t\t\tif k > m+1:\n",
    "\t\t\t\t\tH_next=apriori_gen(H_curr)\n",
    "\t\t\t\t\tto_remove=[]\n",
    "\t\t\t\t\tfor h in H_next:\n",
    "\t\t\t\t\t\tX=tuple(sorted(set(itemset)-set(h)))\n",
    "\t\t\t\t\t\tY=tuple(sorted(h))\n",
    "\t\t\t\t\t\tconfidence = support / (frequent_items[k-m-2][X])\n",
    "\t\t\t\t\t\tif confidence>MIN_CONF:\n",
    "\t\t\t\t\t\t\trule=[]\n",
    "\t\t\t\t\t\t\trule.append(X)\n",
    "\t\t\t\t\t\t\trule.append(Y)\n",
    "\t\t\t\t\t\t\trules.append({tuple(rule):confidence})\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tto_remove.append(h)\n",
    "\t\t\t\t\tH_next=[x for x in H_next if x not in to_remove]\n",
    "\t\t\t\t\tH_curr=H_next\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbreak\t\n",
    "\treturn rules\n",
    "\n",
    "def display_rules(rules, frequent_items, write=False):\n",
    "\t'''\n",
    "\tFunction to display and write rules to file in the prescribed format.\n",
    "\tPrescribed Format\n",
    "\t-----------------\n",
    "\tAssociation Rules-\n",
    "\tPrecedent (itemset (support count)) ---> Antecedent (itemset (support count)) - confidence value\n",
    "\t\n",
    "\tFrequent itemsets-\n",
    "\tFrequent itemset (support count)\n",
    "\tParameters\n",
    "\t----------\n",
    "\trules : list\n",
    "\t    list containing all rules generated by generate_rules function.\n",
    "\tfrequent_items : list\n",
    "\t    list containing all frequent itemsets.\n",
    "\twrite : bool\n",
    "\t    write to file if true. Two files are created- association_rules.txt and frequent_itemsets.txt\n",
    "\t'''\n",
    "\treverse_map=pickle.load(open('reverse_map.pkl', 'rb'))\n",
    "\tbad_chars=\"[]''\"\n",
    "\twith open('outputs/association_rules.txt', 'w+') as f:\n",
    "\t\tfor rule in rules:\n",
    "\t\t\tX, Y=list(rule.keys())[0]\n",
    "\t\t\tprecedent_support_count, antecedent_support_count=(frequent_items[len(X)-1][X], frequent_items[len(Y)-1][Y])\n",
    "\t\t\tconfidence=list(rule.values())[0]\n",
    "\t\t\tprint(str([reverse_map[x] for x in X]).strip(bad_chars).replace(\"'\", '')+'('+str(precedent_support_count)+')'+' ---> '+str([reverse_map[y] for y in Y]).strip(bad_chars).replace(\"'\", '') +'('+str(antecedent_support_count)+')' + ' - conf('+ str(confidence)+ ')')\n",
    "\t\t\tf.write(str([reverse_map[x] for x in X]).strip(bad_chars).replace(\"'\", '')+'('+str(precedent_support_count)+')'+' ---> '+str([reverse_map[y] for y in Y]).strip(bad_chars).replace(\"'\", '') +'('+str(antecedent_support_count)+')' + ' - conf('+ str(confidence)+ ')'+'\\n')\n",
    "\n",
    "\twith open('outputs/frequent_itemsets.txt', 'w+') as f:\n",
    "\t\tfor k_itemset in frequent_items:\n",
    "\t\t\tfor itemset, support in k_itemset.items():\n",
    "\t\t\t\tf.write(str([reverse_map[x] for x in itemset]).strip(bad_chars).replace(\"'\", '')+' ('+str(support)+')'+'\\n')\n",
    "\t\t\t\n",
    "if __name__=='__main__':\n",
    "\tdata_path = 'datasets/dishes.csv'\n",
    "\tfrequent_items = frequent_itemset_generation(data_path)\n",
    "\trules = generate_rules(frequent_items)\n",
    "\tdisplay_rules(rules, frequent_items, write=True)\n",
    "\tno_itemsets=0\n",
    "\tfor x in frequent_items:\n",
    "\t\tno_itemsets+=len(x)\n",
    "\tprint('No of rules:',len(rules), 'No of itemsets:', no_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
