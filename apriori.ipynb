{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from hash_tree.ipynb\n",
      "importing Jupyter notebook from timing_wrapper.ipynb\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "import import_ipynb\n",
    "from hash_tree import Tree, generate_subsets\n",
    "from timing_wrapper import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important variables\n",
    "MINSUP = 30 # Minimum support\n",
    "HASH_DENOMINATOR = 10 # Denominator of the hash function\n",
    "MIN_CONF = 0.5# Minimum confidence\n",
    "\n",
    "@timeit\n",
    "def load_data(path):\n",
    "    items=[]\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        transactions = list(reader)\n",
    "    for x in transactions:\n",
    "        items.extend(x)\n",
    "    items=sorted(set(items))\n",
    "    return transactions, items\n",
    "\n",
    "def create_map(items):\n",
    "    \n",
    "    map_ = {x:i for i,x in enumerate(items)}\n",
    "    reverse_map = {i:x for i,x in enumerate(items)}\n",
    "    return map_, reverse_map\n",
    "\n",
    "def applymap(transaction, map_):\n",
    "   \n",
    "    ret = []\n",
    "    for item in transaction:\n",
    "        ret.append(map_[item])\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#@timeit\n",
    "def apriori_gen(l_prev):\n",
    "  \n",
    "    n = len(l_prev)\n",
    "    c_curr = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            temp_a = l_prev[i]\n",
    "            temp_b = l_prev[j]\n",
    "            if temp_a[:-1] == temp_b[:-1]:\n",
    "                temp_c = []\n",
    "                temp_c.extend(temp_a)\n",
    "                temp_c.append(temp_b[-1])\n",
    "                temp_c=sorted(temp_c)\n",
    "                c_curr.append(temp_c)\n",
    "    return c_curr\n",
    "\n",
    "@timeit\n",
    "def subset(c_list, transactions):\n",
    "\n",
    "    candidate_counts={}\n",
    "    t=Tree(c_list, k=HASH_DENOMINATOR, max_leaf_size=100)\n",
    "    for transaction in transactions:\n",
    "        subsets =generate_subsets(transaction, len(c_list[0]))\n",
    "        for sub in subsets:\n",
    "            t.check(sub, update=True)\n",
    "    for candidate in c_list:\n",
    "        candidate_counts[tuple(candidate)] = t.check(candidate, update=False)\n",
    "    return candidate_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_itemset_generation(data_path):\n",
    "    # Uncomment the following lines to load saved pickle file and avoid the extra time required\n",
    "    # for frequent itemset generation.\n",
    "    # if 'l_final.pkl' in os.listdir('.'):\n",
    "    # return pickle.load(open('l_final.pkl', 'rb'))\n",
    "    transactions, items = load_data(data_path)\n",
    "    map_, reverse_map = create_map(items)\n",
    "    pickle.dump(reverse_map, open('reverse_map.pkl', 'wb+'))\n",
    "    one_itemset = [[itemset] for itemset in items]\n",
    "    items_mapped = [applymap(itemset, map_) for itemset in one_itemset]\n",
    "    transactions_mapped = [applymap(transaction, map_) for transaction in transactions]\n",
    "    temp_l_current = subset(items_mapped, transactions_mapped)\n",
    "    l_current={}\n",
    "    for t in temp_l_current.keys():\n",
    "        if temp_l_current[t] > MINSUP:\n",
    "            l_current[tuple(t)] = temp_l_current[t]\n",
    "    L_final = []\n",
    "    L_final.append(l_current)\n",
    "\n",
    "   \n",
    "    while(len(l_current)):\n",
    "        c_current = apriori_gen(list(l_current.keys()))\n",
    "        if len(c_current):\n",
    "            C_t = subset(c_current, transactions_mapped)\n",
    "            l_current = {}\n",
    "            for c in C_t.keys():\n",
    "                if C_t[c] > MINSUP:\n",
    "                    l_current[tuple(sorted(c))] = C_t[c]\n",
    "            if len(l_current):\n",
    "                L_final.append(l_current)\n",
    "        else:\n",
    "            break\n",
    "    pickle.dump(L_final, open('l_final.pkl', 'wb+'))\n",
    "    return L_final\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(frequent_items):\n",
    "    rules=[]\n",
    "    for k_itemset in frequent_items:\n",
    "        k=len(list(k_itemset.keys())[0])\n",
    "        if k==1: # No rules can be generated using 1 itemsets\n",
    "            continue\n",
    "        for itemset, support in k_itemset.items():\n",
    "            H_curr=[[x] for x in itemset]\n",
    "            to_remove=[]\n",
    "            for h in H_curr:\n",
    "                X=tuple(sorted(set(itemset)-set(h)))\n",
    "                Y=tuple(sorted(h))\n",
    "                confidence = support / (frequent_items[k-2][X])\n",
    "                if confidence > MIN_CONF:\n",
    "                    rule=[]\n",
    "                    rule.append(X)\n",
    "                    rule.append(Y)\n",
    "                    rules.append({tuple(rule):confidence})\n",
    "                else:\n",
    "                    to_remove.append(h)\n",
    "\n",
    "            H_curr=[x for x in H_curr if x not in to_remove]\n",
    "\n",
    "            for m in range(1,k-1):\n",
    "                if k > m+1:\n",
    "                    H_next=apriori_gen(H_curr)\n",
    "                    to_remove=[]\n",
    "                    for h in H_next:\n",
    "                        X=tuple(sorted(set(itemset)-set(h)))\n",
    "                        Y=tuple(sorted(h))\n",
    "                        confidence = support / (frequent_items[k-m-2][X])\n",
    "                        if confidence>MIN_CONF:\n",
    "                            rule=[]\n",
    "                            rule.append(X)\n",
    "                            rule.append(Y)\n",
    "                            rules.append({tuple(rule):confidence})\n",
    "                        else:\n",
    "                            to_remove.append(h)\n",
    "                    H_next=[x for x in H_next if x not in to_remove]\n",
    "                    H_curr=H_next\n",
    "                else:\n",
    "                    break\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rules(rules, frequent_items, write=False):\n",
    "    reverse_map=pickle.load(open('reverse_map.pkl', 'rb'))\n",
    "    bad_chars=\"[]''\"\n",
    "    with open('outputs/association_rules.txt', 'w+') as f:\n",
    "        for rule in rules:\n",
    "            X, Y=list(rule.keys())[0]\n",
    "            precedent_support_count, antecedent_support_count=(frequent_items[len(X)-1][X], frequent_items[len(Y)-1][Y])\n",
    "            confidence=list(rule.values())[0]\n",
    "            print(str([reverse_map[x] for x in X]).strip(bad_chars).replace(\"'\", '')+'('+str(precedent_support_count)+')'+' ---> '+str([reverse_map[y] for y in Y]).strip(bad_chars).replace(\"'\", '') +'('+str(antecedent_support_count)+')' + ' - conf('+ str(confidence)+ ')')\n",
    "            f.write(str([reverse_map[x] for x in X]).strip(bad_chars).replace(\"'\", '')+'('+str(precedent_support_count)+')'+' ---> '+str([reverse_map[y] for y in Y]).strip(bad_chars).replace(\"'\", '') +'('+str(antecedent_support_count)+')' + ' - conf('+ str(confidence)+ ')'+'\\n')\n",
    "\n",
    "    with open('outputs/frequent_itemsets.txt', 'w+') as f:\n",
    "        for k_itemset in frequent_items:\n",
    "            for itemset, support in k_itemset.items():\n",
    "                f.write(str([reverse_map[x] for x in itemset]).strip(bad_chars).replace(\"'\", '')+' ('+str(support)+')'+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/dishes.csv\n",
      "load_data took 0.031502723693847656 seconds.\n",
      "subset took 0.5302462577819824 seconds.\n",
      "subset took 32.56483864784241 seconds.\n",
      "subset took 1.7055048942565918 seconds.\n",
      "subset took 0.35089588165283203 seconds.\n",
      " Beef Burger(51) ---> Burgers(541) - conf(0.7647058823529411)\n",
      " Veg Burger(69) --->  Burgers(775) - conf(0.5072463768115942)\n",
      " Chicken Boneless Biryani(70) --->  Vegetable Biryani(268) - conf(0.5428571428571428)\n",
      " Chicken Burger(89) --->  Pasta(1265) - conf(0.5393258426966292)\n",
      "Shawarma(82) --->  Chicken Grill(204) - conf(0.524390243902439)\n",
      " Thukpa(51) --->  Chicken Momo(80) - conf(0.6078431372549019)\n",
      " Chicken Roll(80) ---> Rolls(204) - conf(0.575)\n",
      " Craft Beer(88) --->  Cocktails(823) - conf(0.6818181818181818)\n",
      " Jumbo Prawns(52) --->  Cocktails(823) - conf(0.7115384615384616)\n",
      " Sangria(120) --->  Cocktails(823) - conf(0.6083333333333333)\n",
      "Beer(151) --->  Cocktails(823) - conf(0.5761589403973509)\n",
      " Kesari Bath(47) --->  Filter Coffee(190) - conf(0.6595744680851063)\n",
      " Wings(56) --->  Fries(455) - conf(0.5714285714285714)\n",
      " Hot Chocolate Fudge(67) --->  Waffles(241) - conf(0.5373134328358209)\n",
      " Hot Dog(47) ---> Burgers(541) - conf(0.723404255319149)\n",
      "Vada(62) --->  Idli(164) - conf(0.5161290322580645)\n",
      "Vada(62) --->  Masala Dosa(259) - conf(0.6774193548387096)\n",
      "Hyderabadi Biryani(61) --->  Mutton Biryani(407) - conf(0.5573770491803278)\n",
      " Prawn Ghee Roast(48) --->  Neer Dosa(162) - conf(0.6666666666666666)\n",
      " Neer Dosa(162) ---> Sea Food(182) - conf(0.5061728395061729)\n",
      " Beer,  Pizza(78) --->  Nachos(573) - conf(0.5256410256410257)\n",
      " Beer,  Nachos(73) --->  Pizza(834) - conf(0.5616438356164384)\n",
      " Burgers,  Cocktails(74) --->  Pasta(1265) - conf(0.581081081081081)\n",
      " Burgers, Cocktails(58) --->  Nachos(573) - conf(0.5344827586206896)\n",
      " Burgers,  Sandwich(80) --->  Pasta(1265) - conf(0.5875)\n",
      " Burgers, Cocktails(58) --->  Pizza(834) - conf(0.5862068965517241)\n",
      " Cappuccino,  Sandwiches(58) ---> Coffee(441) - conf(0.5344827586206896)\n",
      " Chicken Biryani,  Mocktails(46) --->  Cocktails(823) - conf(0.7391304347826086)\n",
      " Chicken Biryani,  Cocktails(53) --->  Mocktails(692) - conf(0.6415094339622641)\n",
      " Cocktails,  Nachos(108) --->  Mocktails(692) - conf(0.5462962962962963)\n",
      " Pizza, Beer(64) --->  Cocktails(823) - conf(0.734375)\n",
      " Cocktails, Beer(87) --->  Pizza(834) - conf(0.5402298850574713)\n",
      " Coffee, Pizza(39) --->  Pasta(1265) - conf(0.8461538461538461)\n",
      " Fries, Cocktails(34) --->  Pizza(834) - conf(0.9117647058823529)\n",
      " Mocktails, Pizza(49) --->  Pasta(1265) - conf(0.7755102040816326)\n",
      " Nachos, Pizza(86) --->  Pasta(1265) - conf(0.6046511627906976)\n",
      " Nachos, Cocktails(67) --->  Pizza(834) - conf(0.5373134328358209)\n",
      " Pizza, Coffee(57) --->  Pasta(1265) - conf(0.6842105263157895)\n",
      " Sandwiches, Pizza(62) --->  Pasta(1265) - conf(0.5645161290322581)\n",
      " Sandwiches, Pasta(60) --->  Sandwich(401) - conf(0.6)\n",
      " Sandwich, Pasta(41) --->  Sandwiches(639) - conf(0.8780487804878049)\n",
      "No of rules: 41 No of itemsets: 740\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    data_path = 'datasets/dishes.csv'\n",
    "    print(data_path)\n",
    "    frequent_items = frequent_itemset_generation(data_path)\n",
    "    rules = generate_rules(frequent_items)\n",
    "    display_rules(rules, frequent_items, write=True)\n",
    "    no_itemsets=0\n",
    "    for x in frequent_items:\n",
    "        no_itemsets+=len(x)\n",
    "print('No of rules:',len(rules), 'No of itemsets:', no_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
